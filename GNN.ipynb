{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2052,
     "status": "ok",
     "timestamp": 1608248071388,
     "user": {
      "displayName": "Mai Mayeg Abdelaal",
      "photoUrl": "",
      "userId": "06466154604659492000"
     },
     "user_tz": -60
    },
    "id": "-jkvtv-GJjPb",
    "outputId": "f3a22a99-c0ec-4f1f-ca6a-f4dfa41d104a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1608253986894,
     "user": {
      "displayName": "Valdimar Eggertsson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjioAoAH10B9GvB1XG0gmXQTQZOP5T8Me9EivDR=s64",
      "userId": "16856514618820400921"
     },
     "user_tz": 0
    },
    "id": "9X5OFbUkJxiT",
    "outputId": "880a2e5c-a798-4060-8f45-3a14eaff0270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6], [7]]\n"
     ]
    }
   ],
   "source": [
    "path_to_glove = '/content/drive/MyDrive/' + 'glove.840B.300d.txt'\n",
    "l = [1,2,3,4,5,6,7]\n",
    "n = 3\n",
    "print([l[i:i + n] for i in range(0, len(l), n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "M1ghhRbYb2w6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "import logging\n",
    "\n",
    "from gensim import utils\n",
    "import nltk\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shutil\n",
    "import smart_open\n",
    "from sys import platform\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ys1nNpoTPYKO"
   },
   "source": [
    "This is just for creating the gensim file.\n",
    "\n",
    "It will be saved into Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LY7a3ubCrOg9"
   },
   "source": [
    "After the glove_2.2M.txt has been written, save it to Drive to load later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iFxM-bSPNjvz"
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('data\\\\glove_2.2M.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSIDkSD4o5Xg"
   },
   "source": [
    "### Basic functions, for processing the data and building a graph\n",
    "\n",
    "\n",
    "\n",
    "- get_words\n",
    "\n",
    "- capitalize(word), low_case(word)\n",
    "\n",
    "- infer_vector_from_word\n",
    "\n",
    "- infer_vector_from_doc\n",
    "\n",
    "- get_vectors_from_nodes_in_graph\n",
    "\n",
    "- get_types_from_nodes_in_graph\n",
    "\n",
    "- get_edge_name_with_signature\n",
    "\n",
    "- get_node_name_with_signature\n",
    "\n",
    "- add_triplets_to_graph_bw\n",
    "\n",
    "- plot_graph\n",
    "\n",
    "- get_chunks\n",
    "\n",
    "- bin_data_into_buckets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ozBPN6qlb2xQ"
   },
   "outputs": [],
   "source": [
    "_is_relevant = [.0, 1.]\n",
    "_is_not_relevant = [1., 0.]\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_words(text):\n",
    "    '''Use: tokenised = get_words(text)\n",
    "  Pre: \n",
    "    text is a string\n",
    "  Post: \n",
    "    words is a list of the words in the text''' \n",
    "    tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "    words = tokenizer.tokenize(utils.to_unicode(text))\n",
    "    return words\n",
    "\n",
    "\n",
    "def capitalize(word):\n",
    "    return word[0].upper() + word[1:]\n",
    "\n",
    "\n",
    "def low_case(word):\n",
    "    return word[0].lower() + word[1:]\n",
    "\n",
    "\n",
    "def infer_vector_from_word(model, word):\n",
    "    '''Use: vector = infer_vector_from_word(model,word)\n",
    "  Pre:\n",
    "      model is models.keyedvectors from gensim (a mapping between keys and vectors)\n",
    "      word is a key for model, such as a word from the vocabulary\n",
    "  Post:\n",
    "      vector is a gensim model vector representation, such as glove embedding, for word'''  \n",
    "    vector = np.zeros(300)\n",
    "    try:\n",
    "        vector = model[word]\n",
    "    except:\n",
    "        try:\n",
    "            vector = model[capitalize(word)]\n",
    "        except:\n",
    "            try:\n",
    "                vector = model[low_case(word)]\n",
    "            except:\n",
    "                pass\n",
    "    return vector\n",
    "\n",
    "\n",
    "def infer_vector_from_doc(model, text):\n",
    "    '''Use: vector = infer_vector_from_word(model,text)\n",
    "    Pre:\n",
    "        model is models.keyedvectors from gensim (a mapping between keys and vectors)\n",
    "        text is a key for model, in this case a document (string of words)\n",
    "    Post:\n",
    "        vector is a gensim model vector representation, such as glove embedding, for text'''\n",
    "    words = get_words(text)\n",
    "    vector = np.zeros(300)\n",
    "    for word in words:\n",
    "        vector += infer_vector_from_word(model, word)\n",
    "    norm = np.linalg.norm(vector)\n",
    "    if norm > 0:\n",
    "        vector /= norm\n",
    "    return vector\n",
    "\n",
    "\n",
    "def get_vectors_from_nodes_in_graph(g, model):\n",
    "    '''Use: vectors = get_vectors_from_nodes_in_graph(graph, model)\n",
    "    Pre:\n",
    "        graph is a networkx graph\n",
    "        model is models.keyedvectors from gensim (a mapping between keys and vectors)\n",
    "    Post:\n",
    "        vector is a numpy array of gensim model vector representations, such as glove embedding, for the text corresponding to each node in graph'''\n",
    "    nodes = nx.nodes(g)\n",
    "    vectors = []\n",
    "    for node in nodes:\n",
    "        text = node.replace('_', ' ')\n",
    "        text = text.split('|')[0]\n",
    "        vectors.append(infer_vector_from_doc(model, text))\n",
    "    return np.array(vectors)\n",
    "\n",
    "\n",
    "def get_types_from_nodes_in_graph(g):\n",
    "    '''Use: vectors = get_types_from_nodes_in_graph(graph)\n",
    "    Pre:\n",
    "        graph is a networkx graph\n",
    "    Post:\n",
    "        vectors is a binary numpy array, with 1 if a node is a vertex and 0 if a node is an edge'''\n",
    "    nodes = nx.nodes(g)\n",
    "    vectors = []\n",
    "    for node in nodes:\n",
    "        texts = node.split('|')\n",
    "        vector = np.zeros(3)\n",
    "        if 'NODE' in texts:\n",
    "            vector[0] = 1.\n",
    "        if 'EDGE' in texts:\n",
    "            vector[1] = 1.\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Vy2zGyZEqIOu"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_edge_name_with_signature(node_str):\n",
    "    node_str = node_str.split('|')[0].lower()\n",
    "    node_str += '|EDGE'\n",
    "    return node_str\n",
    "\n",
    "\n",
    "def get_node_name_with_signature(node_str):\n",
    "    node_str = node_str.split('|')[0].lower()\n",
    "    node_str += '|NODE'\n",
    "    return node_str\n",
    "\n",
    "\n",
    "def add_triplets_to_graph_bw(g, triplets):\n",
    "    '''Use:  g = add_triplets_to_graph_bw(graph, triplets)\n",
    "    Pre:\n",
    "        graph is a networkx graph\n",
    "        triplets is a list of triplets (node1, relation, node2)\n",
    "    Post:\n",
    "        g is graph with the entities and relations added to it'''\n",
    "    for n1, r, n2 in triplets:\n",
    "        clean_n1 = get_node_name_with_signature(n1)\n",
    "        clean_n2 = get_node_name_with_signature(n2)\n",
    "        clean_r = get_edge_name_with_signature(r)\n",
    "        g.add_node(clean_n1)\n",
    "        g.add_node(clean_n2)\n",
    "        g.add_node(clean_r)\n",
    "        g.add_edge(clean_n2, clean_r, **{'label': 'to_relation'})\n",
    "        g.add_edge(clean_r, clean_n1, **{'label': 'to_node'})\n",
    "    return g\n",
    "\n",
    "\n",
    "def plot_graph(g):\n",
    "    layout = nx.shell_layout(g)\n",
    "    nx.draw_networkx(g, pos=layout)\n",
    "    nx.draw_networkx_edge_labels(g, pos=layout)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_chunks(l, n):\n",
    "    '''Use: partitioned = get_chunks(l,n)\n",
    "    Pre:\n",
    "        l is a list\n",
    "        n > 0\n",
    "    Post:\n",
    "        partitioned is a list of the elements of l grouped together in chunks of size n'''\n",
    "    return [l[i:i + n] for i in range(0, len(l), n)]\n",
    "\n",
    "\n",
    "def bin_data_into_buckets(data, batch_size):\n",
    "    '''Use: buckets = bin_data_into_buckets(data, batch_size)\n",
    "    Pre:\n",
    "        data is a list with the training data from wikidata-disambig-train.json\n",
    "        batch_size > 0\n",
    "    Post:\n",
    "        buckets is a list of the items in data, broken into batches'''\n",
    "    buckets = []\n",
    "    size_to_data_dict = {}\n",
    "    for item in data:\n",
    "        # mappings between the length of each item in data and the item added to size_to_data_dict\n",
    "        seq_length = len(item['graph']['vectors'])\n",
    "        question_length = len(item['question_vectors'])\n",
    "        try:\n",
    "            size_to_data_dict[(seq_length, question_length)].append(item)\n",
    "        except:\n",
    "            size_to_data_dict[(seq_length, question_length)] = [item]\n",
    "    for key in size_to_data_dict.keys():\n",
    "        # for each (seq_length, question_length), its corresponding item is broken into chunks of size batch_size, added to buckets\n",
    "        data = size_to_data_dict[key]\n",
    "        chunks = get_chunks(data, batch_size)\n",
    "        for chunk in chunks:\n",
    "            # chunks of size batch_size added to the bucket\n",
    "            buckets.append(chunk)\n",
    "    return buckets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pEf1vVSku95"
   },
   "source": [
    "### Functions for getting the graph ready\n",
    "\n",
    "- get_bw_graph\n",
    "\n",
    "- get_adjacency_matrices_and_vectors_given_triplets\n",
    "\n",
    "- convert_text_into_vector_sequence\n",
    "\n",
    "- get_item_mask_for_words\n",
    "\n",
    "\"With the topology of the Wikidata graph, the information of\n",
    "each node is propagated onto the central item. Ideally, after\n",
    "the graph convolutions, the vector at the position of the central\n",
    "item summarizes the information in the graph.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9PGdwNFCLMcy"
   },
   "outputs": [],
   "source": [
    "def get_bw_graph(triplets):\r\n",
    "    '''Use: graph = get_bw_graph(triplets)\r\n",
    "    Pre:\r\n",
    "        triplets is a list of [node1,relation,node2]\r\n",
    "    Post:\r\n",
    "        graph is a networkx directed graph representation of the triplets'''\r\n",
    "    g_bw = nx.DiGraph()\r\n",
    "    add_triplets_to_graph_bw(g_bw, triplets)\r\n",
    "    return g_bw\r\n",
    "\r\n",
    "\r\n",
    "def get_adjacency_matrices_and_vectors_given_triplets(triplets, central_item, model):\r\n",
    "    '''Use: adj_vect_types = get_adjacency_matrices_and_vectors_given_triplets(triplets, central_item, model)\r\n",
    "    Pre:\r\n",
    "        triplets is a list of [node1,relation,node2]\r\n",
    "        central_item is the central node in the graph represented by the triplets\r\n",
    "        model is models.keyedvectors, such as word2vec: a mapping between keys (such as words) and a vector representation\r\n",
    "\r\n",
    "    Post:\r\n",
    "        adj_vect_types is a dict with:\r\n",
    "            the adjacency matrix of the graph, the vector is glove embeddings for each node and its type (whether it is a vertex or edge)'''\r\n",
    "    g_bw = get_bw_graph(triplets)\r\n",
    "\r\n",
    "    vectors = get_vectors_from_nodes_in_graph(g_bw, model)\r\n",
    "    node_types = get_types_from_nodes_in_graph(g_bw)\r\n",
    "    nodelist = list(g_bw.nodes())\r\n",
    "    try:\r\n",
    "        central_node_index = nodelist.index(central_item + '|NODE')\r\n",
    "        nodelist[central_node_index], nodelist[0] = nodelist[0], nodelist[central_node_index]\r\n",
    "    except Exception as e:\r\n",
    "        print('nodelist:', e)\r\n",
    "        raise e\r\n",
    "    A_bw = np.array(nx.to_numpy_matrix(g_bw, nodelist=nodelist))\r\n",
    "    return {'A_bw': A_bw,\r\n",
    "            'vectors': vectors,\r\n",
    "            'types': node_types}\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def convert_text_into_vector_sequence(model, text):\r\n",
    "    '''Use: seq = convert_text_into_vector_sequence(model, text)\r\n",
    "    Pre:\r\n",
    "        model is a word2vec mapping\r\n",
    "        text is a string\r\n",
    "    Post:\r\n",
    "        seq is a list of the embeddings of the words in text'''\r\n",
    "    words = get_words(text)\r\n",
    "    vectors = []\r\n",
    "    for word in words:\r\n",
    "        vectors.append(infer_vector_from_word(model, word))\r\n",
    "    return vectors\r\n",
    "\r\n",
    "\r\n",
    "def get_item_mask_for_words(text, item):\r\n",
    "    '''Use: mask = get_item_mask_for_words(text, item)\r\n",
    "    Pre:\r\n",
    "        text is a string\r\n",
    "        item is a string, name, possibly in text\r\n",
    "    Post:\r\n",
    "        mask is a binary list,  marks where in the text the item is (1 for the name, 0 everywhere else)\r\n",
    "        \"This mask acts as a \"manually induced\" attention of the item to disambiguate for\"'''\r\n",
    "    words = get_words(text)\r\n",
    "    types = []\r\n",
    "    words_in_item = get_words(item.lower())\r\n",
    "    for word in words:\r\n",
    "        types.append([1. if word.lower() in words_in_item else 0.] * 200)\r\n",
    "    return types\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def infer_vector_from_vector_nodes(vector_list):\r\n",
    "    vector = np.zeros(300)\r\n",
    "    for v in vector_list:\r\n",
    "        vector += v\r\n",
    "    norm = np.linalg.norm(vector)\r\n",
    "    if norm > 0:\r\n",
    "        vector /= norm\r\n",
    "    return vector\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8gwSUJnsgV9"
   },
   "source": [
    "### Functions for processing the json data file and generating a graph with the wikidata information\n",
    "\n",
    "- translate_from_url\n",
    "\n",
    "- create_text_item_graph_dict\n",
    "\n",
    "- get_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "levYoNkeb2xU"
   },
   "outputs": [],
   "source": [
    "def translate_from_url(url):\n",
    "    '''Use: item = translate_from_url(url)\n",
    "    Pre:\n",
    "        u is a string,  part of some wikidata url like the id\n",
    "    Post:\n",
    "        item is the data extracted from url (what comes after '/' and before '-')'''\n",
    "    if '/' in url and '-' not in url:\n",
    "\n",
    "        item = url.split('/')[-1]\n",
    "#             print('from wikidata items_1: {}'.format(item))\n",
    "    elif '/' in url and '-' in url:\n",
    "        item = url.split('/')[-1].split('-')[0]\n",
    "#             print('from wikidata items_2: {}'.format(item))\n",
    "    else:\n",
    "        item = url\n",
    "#             print('from wikidata items_3: {}'.format(item))\n",
    "    return item\n",
    "\n",
    "def reverse_lookup(word):\n",
    "    return reverse_dict[word.lower()]\n",
    "\n",
    "\n",
    "def create_text_item_graph_dict(text, item, wikidata_id):\n",
    "    '''Use: dict = create_text_item_graph_dict(text, item, wikidata_id)\n",
    "    Pre:\n",
    "\n",
    "    Post: \n",
    "      dict has the information from the graph in a dictionary form \n",
    "      '''\n",
    "\n",
    "    text_item_graph_dict = {}\n",
    "    text_item_graph_dict['text'] = text\n",
    "    text_item_graph_dict['item'] = item\n",
    "    text_item_graph_dict['wikidata_id'] = wikidata_id\n",
    "    text_item_graph_dict['graph'] = get_graph_from_wikidata_id(wikidata_id, item)\n",
    "    # text_item_graph_dict['item_vector'] = infer_vector_from_doc(_model, item)\n",
    "    text_item_graph_dict['item_vector'] = infer_vector_from_vector_nodes(text_item_graph_dict['graph']['vectors'])\n",
    "    text_item_graph_dict['question_vectors'] = convert_text_into_vector_sequence(model, text)\n",
    "    text_item_graph_dict['question_mask'] = get_item_mask_for_words(text, item)\n",
    "    return text_item_graph_dict\n",
    "\n",
    "\n",
    "\n",
    "def get_json_data(json_data):\n",
    "    '''Use: data, * = get_json_data(da)\n",
    "    Pre: \n",
    "        da is a json file that has been loaded\n",
    "    Post:\n",
    "        \n",
    "    '''\n",
    "    data = []\n",
    "    lost=[]\n",
    "    count =1000\n",
    "    for json_item in json_data[:count]:\n",
    "       \n",
    "        try:\n",
    "            text = json_item['text']\n",
    "            item = json_item['string']\n",
    "\n",
    "            wikidata_id = json_item['correct_id']\n",
    "            # print('before problem {}'.format(i))\n",
    "            text_item_graph_dict = create_text_item_graph_dict(text, item, wikidata_id)\n",
    "#             print('before problem if not with graph {}'.format(i))\n",
    "            text_item_graph_dict['answer'] = _is_relevant\n",
    "#             print('before problem if not with answer {}'.format(i))\n",
    "            data.append(text_item_graph_dict)\n",
    "\n",
    "            wikidata_id = json_item['wrong_id']\n",
    "            text_item_graph_dict = create_text_item_graph_dict(text, item, wikidata_id)\n",
    "            text_item_graph_dict['answer'] = _is_not_relevant\n",
    "            \n",
    "            \n",
    "\n",
    "            data.append(text_item_graph_dict)\n",
    "            \n",
    "        except Exception as e:\n",
    "#             print(str(e))\n",
    "            lost.append(json_item)\n",
    "    return data, lost\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_graph_from_wikidata_id(wikidata_id, central_item):\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = requests.get(url, params={'query': _query % wikidata_id,\n",
    "                                     'format': 'json'}).json()\n",
    "    triplets = []\n",
    "    for item in data['results']['bindings']:\n",
    "        try:\n",
    "            from_item = translate_from_url(wikidata_id)\n",
    "            relation = translate_from_url(item['rel']['value'])\n",
    "            to_item = translate_from_url(item['item']['value'])\n",
    "            triplets.append((from_item, relation, to_item))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            from_item = translate_from_url(item['item']['value'])\n",
    "            relation = translate_from_url(item['rel2']['value'])\n",
    "            to_item = translate_from_url(item['to_item']['value'])\n",
    "            triplets.append((from_item, relation, to_item))\n",
    "        except:\n",
    "            pass\n",
    "    triplets = sorted(list(set(triplets)))\n",
    "    if not triplets:\n",
    "        raise RuntimeError(\"This graph contains no suitable triplets.\")\n",
    "    return get_adjacency_matrices_and_vectors_given_triplets(triplets, central_item, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mISmcn77b2xs"
   },
   "outputs": [],
   "source": [
    "_query = '''\n",
    "SELECT ?rel ?item ?rel2 ?to_item {\n",
    "  wd:%s ?rel ?item\n",
    "  OPTIONAL { ?item ?rel2 ?to_item }\n",
    "  FILTER regex (str(?item), '^((?!statement).)*$') .\n",
    "  FILTER regex (str(?item), '^((?!https).)*$') .\n",
    "} LIMIT 1500\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJvYxjU8b2xt"
   },
   "outputs": [],
   "source": [
    "#  don't run that\r\n",
    "#  graph= get_graph_from_wikidata_id('Q534153','captain marvel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNDs4OUYb2xv"
   },
   "outputs": [],
   "source": [
    "# print(nx.info(graph))\n",
    "# print(triplets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IvVI25Cb2xy"
   },
   "outputs": [],
   "source": [
    "# don't run that\r\n",
    "# nodelist = list(graph.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWvMqjvIb2xz"
   },
   "outputs": [],
   "source": [
    "# don't run that\n",
    "# edges_list = graph.edges()\n",
    "# pos = nx.spring_layout(graph)\n",
    "# plt.figure(num=None, figsize=(20, 20), dpi=80)\n",
    "# plt.axis('off')\n",
    "# fig = plt.figure(1)\n",
    "\n",
    "# nx.draw_networkx_nodes(graph, pos, cmap=plt.get_cmap('jet'),  node_size = 500)\n",
    "\n",
    "# nx.draw_networkx_edges(graph, pos, edgelist=edges_list, edge_color='r', arrows=True)\n",
    "# plt.savefig('captain_marvel_graph.jpg',bbox_inches=\"tight\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tvw6QSout1Ss"
   },
   "source": [
    "### Functions for training the GCN using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "l1gxccC_b2x1"
   },
   "outputs": [],
   "source": [
    "def compute_new_adjacency_matrix(embedding_size, attention_size, memory_dim, A, H, question_vector, name):\n",
    "    Wa = tf.Variable(tf.random.uniform([embedding_size + memory_dim, attention_size], -_rw, _rw),\n",
    "                     name='Wa_' + name)\n",
    "    ba = tf.Variable(tf.random.uniform([attention_size], -_rw, _rw), name='b0_fw' + name)\n",
    "    #problem with tf.nn\n",
    "    WHQ_projection = lambda x: tf.nn.relu(tf.matmul(tf.concat([x, question_vector], axis=1), Wa) + ba)\n",
    "    WHQ = tf.map_fn(WHQ_projection, H)\n",
    "    WHQ = tf.transpose(WHQ, perm=[1, 0, 2])\n",
    "    #problem with tf.nn\n",
    "    WHQ_squared_projection = lambda x: tf.nn.softmax(tf.matmul(x, tf.transpose(x, perm=[1, 0])))\n",
    "    WHQ_squared = tf.map_fn(WHQ_squared_projection, WHQ)\n",
    "    new_A = tf.multiply(A, WHQ_squared)\n",
    "    return new_A\n",
    "\n",
    "\n",
    "def GCN_layer_fw(embedding_size, hidden_layer1_size, memory_dim, hidden, Atilde_fw, question_vector, name):\n",
    "    new_A = compute_new_adjacency_matrix(embedding_size, 250, memory_dim, Atilde_fw, hidden, question_vector, name)\n",
    "    W0_fw = tf.Variable(tf.random.uniform([embedding_size, hidden_layer1_size], -_rw, _rw),\n",
    "                        name='W0_fw' + name)\n",
    "    b0_fw = tf.Variable(tf.random.uniform([hidden_layer1_size], -_rw, _rw), name='b0_fw' + name)\n",
    "    left_X1_projection_fw = lambda x: tf.matmul(x, W0_fw) + b0_fw\n",
    "    left_X1_fw = tf.map_fn(left_X1_projection_fw, hidden)\n",
    "    left_X1_fw = tf.transpose(left_X1_fw, perm=[1, 0, 2], name='left_X1_fw' + name)\n",
    "    #problem with tf.nn\n",
    "    X1_fw = tf.nn.relu(tf.matmul(new_A, left_X1_fw))\n",
    "    X1_fw = tf.transpose(X1_fw, perm=[1, 0, 2])\n",
    "    return X1_fw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "tYjp-Jq4b2x2"
   },
   "outputs": [],
   "source": [
    "TINY = 1e-6\n",
    "ONE = tf.constant(1.)\n",
    "NAMESPACE = 'gcn_qa'\n",
    "forbidden_weight = 1.\n",
    "_weight_for_positive_matches = 1.\n",
    "_rw = 1e-1\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "class GCN_QA(object):\n",
    "    _nodes_vocab_size = 300\n",
    "    _question_vocab_size = 300\n",
    "    _question_vector_size = 150\n",
    "    _types_size = 3\n",
    "    _mask_size = 200\n",
    "    _types_proj_size = 5\n",
    "    _word_proj_size = 50\n",
    "    _word_proj_size_for_rnn = 50\n",
    "    _word_proj_size_for_item = 50\n",
    "    _internal_proj_size = 250\n",
    "    _hidden_layer1_size = 250\n",
    "    _hidden_layer2_size = 250\n",
    "    _output_size = 2\n",
    "\n",
    "    _memory_dim = 100\n",
    "    _stack_dimension = 2\n",
    "\n",
    "    def __init__(self, dropout=1.0):\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        with tf.compat.v1.variable_scope(NAMESPACE):\n",
    "            config = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n",
    "            self.sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "            # Input variables\n",
    "            self.node_X = tf.compat.v1.placeholder(tf.float32, shape=(None, None, self._nodes_vocab_size), name='node_X')\n",
    "            self.types = tf.compat.v1.placeholder(tf.float32, shape=(None, None, self._types_size), name='types')\n",
    "            self.Wt = tf.Variable(tf.random.uniform([self._types_size,\n",
    "                                                     self._types_proj_size], -_rw, _rw))\n",
    "            self.bt = tf.Variable(tf.random.uniform([self._types_proj_size], -_rw, _rw))\n",
    "            #problem tf.nn\n",
    "            self.types_projection = lambda x: tf.nn.relu(tf.matmul(x, self.Wt) + self.bt)\n",
    "            self.types_internal = tf.map_fn(self.types_projection, self.types)\n",
    "            self.question_vectors_fw = tf.compat.v1.placeholder(tf.float32, shape=(None, None, self._question_vocab_size),\n",
    "                                                      name='question_vectors_inp_fw')\n",
    "            self.question_vectors_bw = tf.compat.v1.placeholder(tf.float32, shape=(None, None, self._question_vocab_size),\n",
    "                                                      name='question_vectors_inp_nw')\n",
    "            self.question_mask = tf.compat.v1.placeholder(tf.float32, shape=(None, None, self._mask_size),\n",
    "                                                name='question_mask')\n",
    "\n",
    "            # The question is pre-processed by a bi-GRU\n",
    "            self.Wq = tf.Variable(tf.random.uniform([self._question_vocab_size,\n",
    "                                                     self._word_proj_size_for_rnn], -_rw, _rw))\n",
    "            self.bq = tf.Variable(tf.random.uniform([self._word_proj_size_for_rnn], -_rw, _rw))\n",
    "            self.internal_projection = lambda x: tf.nn.relu(tf.matmul(x, self.Wq) + self.bq)\n",
    "            self.question_int_fw = tf.map_fn(self.internal_projection, self.question_vectors_fw)\n",
    "            self.question_int_bw = tf.map_fn(self.internal_projection, self.question_vectors_bw)\n",
    "            \n",
    "            #problem rnn\n",
    "            self.rnn_cell_fw = tf.compat.v1.nn.rnn_cell.MultiRNNCell([tf.compat.v1.nn.rnn_cell.GRUCell(self._memory_dim) for _ in range(self._stack_dimension)],\n",
    "                                                state_is_tuple=True)\n",
    "            self.rnn_cell_bw = tf.compat.v1.nn.rnn_cell.MultiRNNCell([tf.compat.v1.nn.rnn_cell.GRUCell(self._memory_dim) for _ in range(self._stack_dimension)],\n",
    "                                                state_is_tuple=True)\n",
    "            with tf.compat.v1.variable_scope('fw'):\n",
    "                #problem dynamic rnn\n",
    "                output_fw, state_fw = tf.compat.v1.nn.dynamic_rnn(self.rnn_cell_fw, self.question_int_fw, time_major=True,\n",
    "                                                        dtype=tf.float32)\n",
    "            with tf.compat.v1.variable_scope('bw'):\n",
    "                #problem dynamen rnn\n",
    "                output_bw, state_bw = tf.compat.v1.nn.dynamic_rnn(self.rnn_cell_bw, self.question_int_bw, time_major=True,\n",
    "                                                        dtype=tf.float32)\n",
    "\n",
    "            self.states = tf.concat(values=[output_fw, tf.reverse(output_bw, [0])], axis=2)\n",
    "            self.question_vector_pre = tf.reduce_mean(tf.multiply(self.question_mask, self.states), axis=0)\n",
    "            self.Wqa = tf.Variable(\n",
    "                tf.random.uniform([2 * self._memory_dim, self._question_vector_size], -_rw, _rw),\n",
    "                name='Wqa')\n",
    "            self.bqa = tf.Variable(tf.random.uniform([self._question_vector_size], -_rw, _rw), name='bqa')\n",
    "            #problem tf.nn\n",
    "            self.question_vector = tf.nn.relu(tf.matmul(self.question_vector_pre, self.Wqa) + self.bqa)\n",
    "\n",
    "            # Dense layer before gcn\n",
    "            self.Wi = tf.Variable(tf.random.uniform([self._nodes_vocab_size,\n",
    "                                                     self._word_proj_size], -_rw, _rw))\n",
    "            self.bi = tf.Variable(tf.random.uniform([self._word_proj_size], -_rw, _rw))\n",
    "            self.internal_projection2 = lambda x: tf.nn.relu(tf.matmul(x, self.Wi) + self.bi)\n",
    "            self.word_embeddings = tf.map_fn(self.internal_projection2, self.node_X)\n",
    "\n",
    "            self.inputs = tf.concat(values=[self.word_embeddings, self.types_internal], axis=2)\n",
    "            self.Wp = tf.Variable(tf.random.uniform([self._word_proj_size + self._types_proj_size,\n",
    "                                                     self._internal_proj_size], -_rw, _rw))\n",
    "            self.bp = tf.Variable(tf.random.uniform([self._internal_proj_size], -_rw, _rw))\n",
    "            self.enc_int_projection = lambda x: tf.nn.relu(tf.matmul(x, self.Wp) + self.bp)\n",
    "            self.enc_int = tf.map_fn(self.enc_int_projection, self.inputs)\n",
    "\n",
    "            # GCN part\n",
    "            self.Atilde_fw = tf.nn.dropout(tf.compat.v1.placeholder(tf.float32, shape=(None, None, None), name=\"Atilde_fw\"), 0.25)\n",
    "\n",
    "            self.X1_fw = GCN_layer_fw(self._internal_proj_size,\n",
    "                                      self._hidden_layer1_size,\n",
    "                                      self._question_vector_size,\n",
    "                                      self.enc_int,\n",
    "                                      self.Atilde_fw,\n",
    "                                      self.question_vector,\n",
    "                                      '_1')\n",
    "            #problem tf.nn\n",
    "            self.X1_fw_dropout = tf.nn.dropout(self.X1_fw, dropout)\n",
    "\n",
    "            self.X2_fw = GCN_layer_fw(self._hidden_layer1_size,\n",
    "                                      self._hidden_layer1_size,\n",
    "                                      self._question_vector_size,\n",
    "                                      self.X1_fw_dropout,\n",
    "                                      self.Atilde_fw,\n",
    "                                      self.question_vector,\n",
    "                                      '_2')\n",
    "            self.X2_fw_dropout = tf.nn.dropout(self.X2_fw, dropout)\n",
    "\n",
    "            self.X3_fw = GCN_layer_fw(self._hidden_layer1_size,\n",
    "                                      self._hidden_layer1_size,\n",
    "                                      self._question_vector_size,\n",
    "                                      self.X2_fw_dropout,\n",
    "                                      self.Atilde_fw,\n",
    "                                      self.question_vector,\n",
    "                                      '_3')\n",
    "            self.X3_fw_dropout = tf.nn.dropout(self.X3_fw, dropout)\n",
    "\n",
    "            self.X4_fw = GCN_layer_fw(self._hidden_layer1_size,\n",
    "                                      self._hidden_layer1_size,\n",
    "                                      self._question_vector_size,\n",
    "                                      self.X3_fw_dropout,\n",
    "                                      self.Atilde_fw,\n",
    "                                      self.question_vector,\n",
    "                                      '_4')\n",
    "            #tf.nn\n",
    "            self.X4_fw_dropout = tf.nn.dropout(self.X4_fw, dropout)\n",
    "            self.first_node = self.X4_fw_dropout[0]\n",
    "            self.concatenated = tf.concat(values=[self.question_vector, self.first_node], axis=1)\n",
    "\n",
    "            # Final feedforward layers\n",
    "            self.Ws1 = tf.Variable(\n",
    "                tf.random.uniform([self._question_vector_size\n",
    "                                   + self._hidden_layer1_size,\n",
    "                                   self._hidden_layer2_size], -_rw, _rw),\n",
    "                name='Ws1')\n",
    "            self.bs1 = tf.Variable(tf.random.uniform([self._hidden_layer2_size], -_rw, _rw), name='bs1')\n",
    "            #problem tf.nn\n",
    "            self.first_hidden = tf.nn.relu(tf.matmul(self.concatenated, self.Ws1) + self.bs1)\n",
    "            self.first_hidden_dropout = tf.nn.dropout(self.first_hidden, dropout)\n",
    "\n",
    "            self.Wf = tf.Variable(\n",
    "                tf.random.uniform([self._hidden_layer2_size, self._output_size], -_rw,\n",
    "                                  _rw),\n",
    "                name='Wf')\n",
    "            self.bf = tf.Variable(tf.random.uniform([self._output_size], -_rw, _rw), name='bf')\n",
    "            #problem tf.nn\n",
    "            self.outputs = tf.nn.softmax(tf.matmul(self.first_hidden_dropout, self.Wf) + self.bf)\n",
    "\n",
    "            # Loss function and training\n",
    "            self.y_ = tf.compat.v1.placeholder(tf.float32, shape=(None, self._output_size), name='y_')\n",
    "            self.outputs2 = tf.squeeze(self.outputs)\n",
    "            self.y2_ = tf.squeeze(self.y_)\n",
    "            self.one = tf.ones_like(self.outputs)\n",
    "            self.tiny = self.one * TINY\n",
    "            self.cross_entropy = (tf.reduce_mean(\n",
    "                -tf.reduce_sum(self.y_ * tf.math.log(self.outputs + self.tiny) * _weight_for_positive_matches\n",
    "                               + (self.one - self.y_) * tf.math.log(\n",
    "                    self.one - self.outputs + self.tiny))\n",
    "            ))\n",
    "\n",
    "        # Clipping the gradient\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(1e-4)\n",
    "        gvs = optimizer.compute_gradients(self.cross_entropy)\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs if var.name.find(NAMESPACE) != -1]\n",
    "        self.train_step = optimizer.apply_gradients(capped_gvs)\n",
    "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        # Adding the summaries\n",
    "        tf.summary.scalar('cross_entropy', self.cross_entropy)\n",
    "        self.merged = tf.compat.v1.summary.merge_all()\n",
    "        self.train_writer = tf.compat.v1.summary.FileWriter('./train', self.sess.graph)\n",
    "\n",
    "    def _add_identity(self, A):\n",
    "        num_nodes = A.shape[0]\n",
    "        identity = np.identity(num_nodes)\n",
    "        return identity + A\n",
    "\n",
    "    def __train(self, A_fw, node_X, types, item_vector, question_vectors, question_mask, y):\n",
    "        item_vector = np.array(item_vector)\n",
    "        Atilde_fw = np.array([self._add_identity(item) for item in A_fw])\n",
    "\n",
    "        node_X = np.array(node_X)\n",
    "        node_X = np.transpose(node_X, (1, 0, 2))\n",
    "\n",
    "        types = np.array(types)\n",
    "        types = np.transpose(types, (1, 0, 2))\n",
    "\n",
    "        question_vectors = np.array(question_vectors)\n",
    "        question_vectors_fw = np.transpose(question_vectors, (1, 0, 2))\n",
    "        question_vectors_bw = question_vectors_fw[::-1, :, :]\n",
    "\n",
    "        question_mask = np.array(question_mask)\n",
    "        question_mask = np.transpose(question_mask, (1, 0, 2))\n",
    "\n",
    "        y = np.array(y)\n",
    "\n",
    "        feed_dict = {}\n",
    "        feed_dict.update({self.node_X: node_X})\n",
    "        feed_dict.update({self.types: types})\n",
    "        feed_dict.update({self.question_vectors_fw: question_vectors_fw})\n",
    "        feed_dict.update({self.question_vectors_bw: question_vectors_bw})\n",
    "        feed_dict.update({self.question_mask: question_mask})\n",
    "        feed_dict.update({self.Atilde_fw: Atilde_fw})\n",
    "        feed_dict.update({self.y_: y})\n",
    "\n",
    "        loss, _, summary, outputs2, y2 = self.sess.run(\n",
    "            [self.cross_entropy, self.train_step, self.merged, self.outputs2, self.y2_], feed_dict)\n",
    "        return loss, summary\n",
    "\n",
    "    def train(self, data, epochs=20):\n",
    "        for epoch in range(epochs):\n",
    "            loss, _ = self.__train([data[i][0] for i in range(len(data))],\n",
    "                                   [data[i][1] for i in range(len(data))],\n",
    "                                   [data[i][2] for i in range(len(data))],\n",
    "                                   [data[i][3] for i in range(len(data))],\n",
    "                                   [data[i][4] for i in range(len(data))],\n",
    "                                   [data[i][5] for i in range(len(data))],\n",
    "                                   [data[i][6] for i in range(len(data))])\n",
    "            print(loss)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    def __predict(self, A_fw, node_X, types, item_vector, question_vectors, question_mask):\n",
    "        item_vector = np.array(item_vector)\n",
    "        Atilde_fw = np.array([self._add_identity(item) for item in A_fw])\n",
    "\n",
    "        node_X = np.array(node_X)\n",
    "        node_X = np.transpose(node_X, (1, 0, 2))\n",
    "\n",
    "        types = np.array(types)\n",
    "        types = np.transpose(types, (1, 0, 2))\n",
    "\n",
    "        question_vectors = np.array(question_vectors)\n",
    "        question_vectors_fw = np.transpose(question_vectors, (1, 0, 2))\n",
    "        question_vectors_bw = question_vectors_fw[::-1, :, :]\n",
    "\n",
    "        question_mask = np.array(question_mask)\n",
    "        question_mask = np.transpose(question_mask, (1, 0, 2))\n",
    "\n",
    "        feed_dict = {}\n",
    "        feed_dict.update({self.node_X: node_X})\n",
    "        feed_dict.update({self.types: types})\n",
    "        feed_dict.update({self.question_vectors_fw: question_vectors_fw})\n",
    "        feed_dict.update({self.question_vectors_bw: question_vectors_bw})\n",
    "        feed_dict.update({self.question_mask: question_mask})\n",
    "        feed_dict.update({self.Atilde_fw: Atilde_fw})\n",
    "\n",
    "        y_batch = self.sess.run([self.outputs2], feed_dict)\n",
    "        return y_batch\n",
    "\n",
    "    def __standardize_item(self, item):\n",
    "        if item[0] < item[1]:\n",
    "            return [0., 1.]\n",
    "        return [1., 0.]\n",
    "\n",
    "    def predict(self, A_fw, node_X, types, item_vector, question_vectors, question_mask):\n",
    "        output = self.__predict([A_fw], [node_X], [types], [item_vector], [question_vectors], [question_mask])\n",
    "        return self.__standardize_item(output[0])\n",
    "\n",
    "    # Loading and saving functions\n",
    "\n",
    "    def save(self, filename):\n",
    "        saver =  tf.compat.v1.train.Saver()\n",
    "        saver.save(self.sess, filename)\n",
    "\n",
    "    def load_tensorflow(self, filename):\n",
    "        saver =  tf.compat.v1.train.Saver([v for v in tf.global_variables() if NAMESPACE in v.name])\n",
    "        saver.restore(self.sess, filename)\n",
    "\n",
    "    @classmethod\n",
    "    def load(self, filename, dropout=1.0):\n",
    "        model = GCN_QA(dropout)\n",
    "        model.load_tensorflow(filename)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1h9EVqGfuGdD"
   },
   "source": [
    "### The function for training and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "p9UX_bH1b2x3"
   },
   "outputs": [],
   "source": [
    "def train(data, model, saving_dir, name_prefix, epochs=20, bucket_size=10, trace_every=1):\n",
    "    import random\n",
    "    import sys\n",
    "\n",
    "    buckets = bin_data_into_buckets(data, bucket_size)\n",
    "    for i in range(epochs):\n",
    "        random_buckets = sorted(buckets, key=lambda x: random.random())\n",
    "        sys.stderr.write('--------- Epoch ' + str(i) + ' ---------\\n')\n",
    "        for bucket in random_buckets:\n",
    "            graph_bucket = []\n",
    "            try:\n",
    "                for item in bucket:\n",
    "                    node_vectors = item['graph']['vectors']\n",
    "                    types = item['graph']['types']\n",
    "                    A_bw = item['graph']['A_bw']\n",
    "                    y = item['answer']\n",
    "                    item_vector = item['item_vector']\n",
    "                    question_vectors = item['question_vectors']\n",
    "                    question_mask = item['question_mask']\n",
    "                    graph_bucket.append((A_bw, node_vectors, types, item_vector, question_vectors, question_mask, y))\n",
    "                if len(graph_bucket) > 0:\n",
    "                    model.train(graph_bucket, 1)\n",
    "            except Exception as e:\n",
    "                print('Exception caught during training: ' + str(e))\n",
    "        if i % trace_every == 0:\n",
    "            save_filename = saving_dir + name_prefix + '-' + str(i) + '.tf'\n",
    "            sys.stderr.write('Saving into ' + save_filename + '\\n')\n",
    "            model.save(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3374,
     "status": "ok",
     "timestamp": 1608249798650,
     "user": {
      "displayName": "Mai Mayeg Abdelaal",
      "photoUrl": "",
      "userId": "06466154604659492000"
     },
     "user_tz": -60
    },
    "id": "OD87YspUb2x3",
    "outputId": "71d3f39f-b278-47f6-9e82-5e97dbbf4b7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodelist: 'nutrition|NODE' is not in list\n",
      "nodelist: 'spanish|NODE' is not in list\n",
      "nodelist: 'south|NODE' is not in list\n",
      "682\n",
      "933\n"
     ]
    }
   ],
   "source": [
    "_bucket_size = 10\n",
    "_minimum_trace = 10\n",
    "with open(os.path.join('../../dataset/wikidata-disambig-train.json')) as f:\n",
    "    json_data = json.load(f)\n",
    "data, lost = get_json_data(json_data)\n",
    "\n",
    "print(len(data))\n",
    "print(len(lost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-6a000824bb82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../dataset/data.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../dataset/lost.p'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pickle.dump(data, open('../../dataset/data.p', 'wb'))\n",
    "pickle.dump(lost, open('../../dataset/lost.p','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "executionInfo": {
     "elapsed": 1034,
     "status": "error",
     "timestamp": 1608249087656,
     "user": {
      "displayName": "Mai Mayeg Abdelaal",
      "photoUrl": "",
      "userId": "06466154604659492000"
     },
     "user_tz": -60
    },
    "id": "gEpfKmdzb2x4",
    "outputId": "e858065c-1b67-4852-8726-257f10779485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n",
      "Exception caught during training: Fetch argument None has invalid type <class 'NoneType'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-17d99bd6915c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m       \u001b[0mbucket_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m       \u001b[0mtrace_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m       )\n",
      "\u001b[1;32m<ipython-input-23-fc8d07aabe1a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data, model, saving_dir, name_prefix, epochs, bucket_size, trace_every)\u001b[0m\n\u001b[0;32m     20\u001b[0m                     \u001b[0mgraph_bucket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_bw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_bucket\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_bucket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Exception caught during training: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-f1c1e97062d8>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data, epochs)\u001b[0m\n\u001b[0;32m    222\u001b[0m                                    \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                                    \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                                    [data[i][6] for i in range(len(data))])\n\u001b[0m\u001b[0;32m    225\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-f1c1e97062d8>\u001b[0m in \u001b[0;36m__train\u001b[1;34m(self, A_fw, node_X, types, item_vector, question_vectors, question_mask, y)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         loss, _, summary, outputs2, y2 = self.sess.run(\n\u001b[1;32m--> 213\u001b[1;33m             [self.cross_entropy, self.train_step, self.merged, self.outputs2, self.y2_], feed_dict)\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Mai Mayeg\\Desktop\\Mai's\\Winpython\\WPy64-3760\\python-3.7.6.amd64\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    958\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 960\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    961\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Mai Mayeg\\Desktop\\Mai's\\Winpython\\WPy64-3760\\python-3.7.6.amd64\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_ref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32mC:\\Users\\Mai Mayeg\\Desktop\\Mai's\\Winpython\\WPy64-3760\\python-3.7.6.amd64\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Loading model: ')\n",
    "_saving_dir = '../data/'\n",
    "nn_model = GCN_QA(dropout=0.25)\n",
    "train(data,\n",
    "      nn_model,\n",
    "      _saving_dir,\n",
    "      name_prefix='qa',\n",
    "      epochs=60,\n",
    "      bucket_size=10,\n",
    "      trace_every=1,\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eg3LcgUUKQEO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJTRgMOJb2x5"
   },
   "outputs": [],
   "source": [
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0N2m2U_rb2x6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
